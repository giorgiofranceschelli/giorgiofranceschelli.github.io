---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: page
title: Research Work
feature_text: |
  # Giorgio Franceschelli
  Researcher at the Department of Computer Science and Engineering, University of Bologna.
# feature_image: "https://picsum.photos/1300/400?image=989"
permalink: /papers/
---

#### Papers

* Francesco Calcagno, Luca Serfilippi, Giorgio Franceschelli, Marco Garavelli, Mirco Musolesi and Ivan Rivalta. 2025. Quantum Chemistry Driven Molecular Inverse Design with Data-free Reinforcement Learning. [arXiv:2503.12653 [physics.chem-ph]](https://arxiv.org/abs/2503.12653) <details style='font-size:80%; text-align:justify'>The inverse design of molecules has challenged chemists for decades. In the past years, machine learning and artificial intelligence have emerged as new tools to generate molecules tailoring desired properties, but with the limit of relying on models that are pretrained on large datasets. Here, we present a data-free generative model based on reinforcement learning and quantum mechanics calculations. To improve the generation, our software is based on a five-model reinforcement learning algorithm designed to mimic the syntactic rules of an original ASCII encoding based on the SMILES one, and here reported. The reinforcement learning generator is rewarded by on-the-fly quantum mechanics calculations within a computational routine addressing conformational sampling. We demonstrate that our software successfully generates new molecules with desired properties finding optimal solutions for problems with known solutions and (sub)optimal molecules for unexplored chemical (sub)spaces, jointly showing significant speed-up to a reference baseline. </details>
* Giorgio Franceschelli and Mirco Musolesi. 2025. DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation. [arXiv:2502.14037 [cs.CL]](https://arxiv.org/abs/2502.14037) <details style='font-size:80%; text-align:justify'>Despite their increasing performance, large language models still tend to reproduce training data, generate several repetitions, and focus on the most common grammatical structures and words. A possible cause is the decoding strategy adopted: the most common ones either consider only the most probable tokens, reducing output diversity, or increase the likelihood of unlikely tokens at the cost of output accuracy and correctness. In this paper, we propose a family of three new decoding methods by leveraging a mathematical analysis of the token probability distribution. In particular, the difference between consecutive, sorted probabilities can be used to avoid incorrect tokens and increase the chance of low-probable but accurate words. Experiments concerning math problem solving, extreme summarization, and the divergent association task show that our approach consistently performs at least as well as current alternatives in terms of quality and diversity.</details>
* Giorgio Franceschelli and Mirco Musolesi. 2025. Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation. [arXiv:2502.13207 [cs.CL]](https://arxiv.org/abs/2502.13207) <details style='font-size:80%; text-align:justify'>Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.</details>
* Giorgio Franceschelli, Claudia Cevenini and Mirco Musolesi. 2024. Training Foundation Models as Data Compression: On Information, Model Weights and Copyright Law. [arXiv:2407.13493 [cs.CY]](https://arxiv.org/abs/2407.13493) <details style='font-size:80%; text-align:justify'>The training process of foundation models as for other classes of deep learning systems is based on minimizing the reconstruction error over a training set. For this reason, they are susceptible to the memorization and subsequent reproduction of training samples. In this paper, we introduce a _training-as-compressing_ perspective, wherein the model's weights embody a compressed representation of the training data. From a copyright standpoint, this point of view implies that the weights could be considered a reproduction or a derivative work of a potentially protected set of works. We investigate the technical and legal challenges that emerge from this framing of the copyright of outputs generated by foundation models, including their implications for practitioners and researchers. We demonstrate that adopting an information-centric approach to the problem presents a promising pathway for tackling these emerging complex legal issues.</details>
* Giorgio Franceschelli and Mirco Musolesi. 2024. Creative Beam Search: LLM-as-a-Judge For Improving Response Generation. _Proc. of the 15th International Conference on Computational Creativity (ICCC'24)_. Jonkoping, Sweden. [PDF](https://computationalcreativity.net/iccc24/short-papers/ICCC24_paper_161.pdf) <details style='font-size:80%; text-align:justify'>Large language models are revolutionizing several areas, including artificial creativity. However, the process of generation in machines profoundly diverges from that observed in humans. In particular, machine generation is characterized by a lack of intentionality and an underlying creative process. We propose a method called Creative Beam Search that uses Diverse Beam Search and LLM-as-a-Judge to perform response generation and response validation. The results of a qualitative experiment show how our approach can provide better output than standard sampling techniques. We also show that the response validation step is a necessary complement to the response generation step.</details>
* Giorgio Franceschelli and Mirco Musolesi. 2024. Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning. [arXiv:2403.07979 [cs.LG]](https://arxiv.org/abs/2403.07979) <details style='font-size:80%; text-align:justify'>The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain. Here, we ask if the same is true for reinforcement learning agents as well. Given limited experience in a real environment, we use imagination-based reinforcement learning to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations. Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments.</details>
* Giorgio Franceschelli and Mirco Musolesi. 2024. Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. _Journal of Artificial Intelligence Research_, 79, 417-446. [doi.org/10.1613/jair.1.15278](https://jair.org/index.php/jair/article/view/15278) <details style='font-size:80%; text-align:justify'>Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.</details>
* Giorgio Franceschelli and Mirco Musolesi. 2023. On the Creativity of Large Language Models. _AI & SOCIETY_, 40, 3785-3795. [doi.org/10.1007/s00146-024-02127-3](https://link.springer.com/article/10.1007/s00146-024-02127-3) <details style='font-size:80%; text-align:justify'>Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion around the dimensions of value, novelty and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press and person. We discuss a set of ``easy'' and ``hard'' problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered by them, the challenges arising by them and the potential associated risks, from both legal and ethical points of view.</details>
* Giorgio Franceschelli and Mirco Musolesi. 2022. DeepCreativity: Measuring Creativity with Deep Learning Techniques. _Intelligenza Artificiale_, 16(2), 151-163. [doi:10.3233/IA-220136](https://content.iospress.com/articles/intelligenza-artificiale/ia220136) <details style='font-size:80%; text-align:justify'>Measuring machine creativity is one of the most fascinating challenges in Artificial Intelligence. This paper explores the possibility of using generative learning techniques for automatic assessment of creativity. The proposed solution does not involve human judgement, it is modular and of general applicability. We introduce a new measure, namely DeepCreativity, based on Margaret Boden’s definition of creativity as composed by value, novelty and surprise. We evaluate our methodology (and related measure) considering a case study, i.e., the generation of 19th century American poetry, showing its effectiveness and expressiveness.</details>
* Giorgio Franceschelli and Mirco Musolesi. 2022. Copyright in Generative Deep Learning. _Data & Policy_, 4, E17. [doi:10.1017/dap.2022.10](https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E#) <details style='font-size:80%; text-align:justify'>Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning (GDL) techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of GDL for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of GDL. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers. </details>
* Giorgio Franceschelli and Mirco Musolesi. 2024. Creativity and Machine Learning: A Survey. _ACM Computing Surveys_, 56(11), 283:1-41. [doi:10.1145/3664595](https://doi.org/10.1145/3664595) <details style='font-size:80%; text-align:justify'>There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.</details>

#### Books and Contributions

* Giorgio Franceschelli, I, Artist. _Opere d’arte e intelligenza artificiale: il curioso caso del diritto d’autore_, Ventura Edizioni, Senigallia, 2019.
* Giorgio Franceschelli, L’importanza del suono nel progresso culturale, in _Prospettive sonore. Percezione e mediazione_, by A. Calanchi e A. Laquidara, Galaad, Giulianova, 2016, pp. 257 - 274.
